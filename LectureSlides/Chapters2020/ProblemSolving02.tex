\section{Assignment 02}
\subsection{}

\begin{frame}{Assignment}
\textit{\color{slidecolor}Question 01:}
\begin{scriptsize}
\begin{equation}
\begin{array}{l}
\left\{ {\left( {\begin{array}{*{20}{c}}
2\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
4
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
5
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
6\\
5
\end{array}} \right)} \right\}\in \omega_1\\
\left\{ {\left( {\begin{array}{*{20}{c}}
5\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
7\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
8\\
5
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
9\\
6
\end{array}} \right)} \right\}\in \omega_2
\end{array}\nonumber
\end{equation}
\end{scriptsize}
\begin{itemize}
\item[(a)] Find the 1-D projection direction that best represents the data points.
\item[(b)] Find out the best direction of the line of projection that maintains the separability of the two classes.
\end{itemize} 
\end{frame}

%\begin{frame}{Assignment}
%\begin{footnotesize}
%\textit{\color{slidecolor}Question:}
%\begin{itemize}
%\item[(a)] Given the following sets of feature vector belonging to two classes $\omega_1$ and $\omega_2$ which is Gaussian distributed.
%\begin{equation}
%(1,2)^t,~(3,5)^t,~(4,3)^t,~(5,6)^t, ~(7,5)^t \in \omega_1 \nonumber
%\end{equation}
%\begin{equation}
%(6,2)^t,~(9,4)^t,~(10,1)^t,~(12,3)^t, ~(13,6)^t \in \omega_2 \nonumber
%\end{equation}
%The vector are projected onto a line to represent the feature vectors by a single feature. Find out the best direction of the line of projection that maintains the separability of the two classes.\\
%\item[(b)] Assuming the mean of the projected point belonging to $\omega_1$ to be the origin of the projection line, identify the point on the projection line that optimally separates two classes. Assume the classes to be equally probable and the projected features also follow Gaussian distribution.
%\end{itemize}
%\end{footnotesize}
%\end{frame}



\begin{frame}{Assignment}
\textit{\color{slidecolor}Question 02:}
\begin{footnotesize}
The following three decision functions are given for a three-class problem.
\begin{align}
{g_1}(x) &= 10{x_1} - {x_2} - 10 = 0 \nonumber\\
{g_2}(x) &= {x_1} + 2{x_2} - 10 = 0 \nonumber\\
{g_3}(x) &= {x_1} - 2{x_2} - 10 = 0 \nonumber
\end{align}
\begin{itemize}
\item [i.] Sketch the decision boundary and regions for each pattern class.
\item [ii.] Assuming that each pattern class is pairwise linearly separable from every other class by a distinct decision surface and letting
\begin{align}
{g_{12}}(x) &= g_1(x) \nonumber\\
{g_{13}}(x) &= g_2(x) \nonumber\\
{g_{23}}(x) &= g_3(x) \nonumber
\end{align}
as listed above, sketch the decision boundary and regions for each pattern class.
\end{itemize}
\end{footnotesize}
\end{frame}


\begin{frame}{Assignment}
\textit{\color{slidecolor}Question 03}\\
\begin{itemize}
\item[(a)] Explain the principle of Fisher linear discriminator.
\item[(b)] Following set of 2-D features vectors from classes A and B are given.
\begin{scriptsize}
\begin{equation}
\begin{array}{l}
\left\{ {\left( {\begin{array}{*{20}{c}}
1\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
1\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
2\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
2\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
{2.5}\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
2\\
2
\end{array}} \right)} \right\} \in A\\
\left\{ {\left( {\begin{array}{*{20}{c}}
3\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
{4.5}\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
4
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
6\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
6
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
7\\
3
\end{array}} \right)} \right\} \in B
\end{array}\nonumber
\end{equation}
\end{scriptsize}
Using rectangular window of size $3\times 3$, compute $p((3.5,3)^t|A)$ and $p((3.5,3)^t|B)$.
\item[(c)] Classify $(3.5,3)^t$ if $P(A)=1/3$ and $P(B)=2/3$.
\end{itemize}
\end{frame}

\begin{frame}{Assignment}
\textit{\color{slidecolor}Question 04:}
The following sets of points are given for a two class problem.
\begin{scriptsize}
\begin{equation}
\begin{array}{l}
\left\{ {\left( {\begin{array}{*{20}{c}}
2\\
6
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
4
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
8
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
6
\end{array}} \right)} \right\} \in \omega_1\\
\left\{ {\left( {\begin{array}{*{20}{c}}
3\\
0
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
1\\
-2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
5\\
-2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
-4
\end{array}} \right)} \right\} \in \omega_2
\end{array}\nonumber
\end{equation}
\end{scriptsize}
Assuming that the feature vectors in every class follow Gaussian distribution and the classes are equally probable, determine the decision boundary between the two classes.
\end{frame}



\begin{frame}{Assignment}
\textit{\color{slidecolor}Question 05:}\\
\begin{footnotesize}
In many pattern classification problems one has the option either to assign the pattern to one of $c$ classes, or to reject it as being unrecognizable. If the cost for rejects is not too high, rejection may be a desirable action. Let
\begin{equation}
\lambda ({\alpha _i}|{\omega _j}) = \left\{ {\begin{array}{*{30}{l}}
0&{i = j~~~~~i,j = 1, \ldots ,c}\\
{{\lambda _r}}&{i = c + 1}\\
{{\lambda _s}}&{\rm otherwise}
\end{array}} \right. \nonumber
\end{equation}
where $\lambda_r$ is the loss incurred for choosing the $(c+1)$th action, rejection, and $\lambda_s$ is the loss incurred for making a substitution error. Show that the minimum risk is obtained if we decide $\omega_i$ if $P(\omega_i|{\rm x})\geq P(\omega_j|{\rm x})$ for all $j$ and if $P(\omega_i|{\rm x})\geq 1- \lambda_r/\lambda_s$, and reject otherwise. What happens if $\lambda_r=0$? What happens if $\lambda_r>\lambda_s$?
\end{footnotesize}
\end{frame}

\begin{frame}{Assignment}
\begin{footnotesize}
\textit{\color{slidecolor}Question 06:}\\
Consider the following set of seven 2-dimensional feature vectors:
\begin{equation}
X_1=(1,0)^t,~X_2=(0,1)^t,~X_3=(0,-1)^t, \nonumber
\end{equation}
\begin{equation}
 ~X_4=(0,0)^t,~X_5=(0,2)^t,~X_6=(0,-2)^t, ~X_7=(-2,0)^t\nonumber
\end{equation}
If $X_1,X_2,X_3\in \omega_1$ and $X_4,X_5,X_6,X_7\in \omega_2$, sketch the decision boundary resulting from the nearest neighbor rule.
\end{footnotesize}
\end{frame}


